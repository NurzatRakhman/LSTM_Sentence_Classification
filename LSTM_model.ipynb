{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "#from tensorflow.models.rnn import rnn \n",
    "from tensorflow.python.ops import rnn\n",
    "\n",
    "\n",
    "#Defining some hyper-params\n",
    "n_hidden = 2       #this is the parameter for input_size in the basic LSTM cell\n",
    "input_size = 2      #n_hidden and input_size will be the same\n",
    "embedding_size = 300\n",
    "\n",
    "batch_size = 50\n",
    "sentence_length = 55\n",
    "num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load as load\n",
    "from functools import reduce\n",
    "\n",
    "#train_set, valid_set, test_set, dic = load.atisfold(3)\n",
    "train_set, test_set, dic = load.atisfull()\n",
    "idx_pad = max(dic['words2idx'].values()) + 1\n",
    "dic['words2idx']['<PAD>'] = idx_pad\n",
    "\n",
    "y_pad = 126\n",
    "\n",
    "idx2label = dict((v,k) for k,v in dic['labels2idx'].items())\n",
    "idx2word = dict((v,k) for k,v in dic['words2idx'].items())\n",
    "\n",
    "train_lex, train_ne, train_y = train_set\n",
    "#valid_lex, valid_ne, valid_y = valid_set\n",
    "test_lex,  test_ne,  test_y  = test_set\n",
    "\n",
    "vocsize = len(set(reduce(lambda x, y: list(x)+list(y),\n",
    "                         train_lex+test_lex))) + 1 # +1 for padding\n",
    "\n",
    "nclasses = len(set(reduce(lambda x, y: list(x)+list(y),\n",
    "                          train_y+test_y)))\n",
    "nsentences = len(train_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_words = map(lambda x: idx2word[x], train_lex[1])     \n",
    "print res_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence = max([len(s) for s in train_lex])\n",
    "print(max_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_lex) + len(test_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentence, pad=-1, max_length=50):\n",
    "    length = len(sentence)\n",
    "    if len(sentence) < max_length:\n",
    "        sentence = np.append(sentence, [pad] * (max_length - length))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\"\"\"\n",
    "each batch is a sentence\n",
    "each batch is a 2D matrix\n",
    "* height: length of the sentence\n",
    "* width: the vocaburary size\n",
    "* x: [n_batch, sentence_length]\n",
    "* y: [n_batch, sentence_length, nclasses]\n",
    "\"\"\"\n",
    "def gen_data(source, s_pad, Y, y_pad, max_length, vocsize, nclasses, n_batch=5):\n",
    "    l = n_batch\n",
    "    for i in range(len(source)):\n",
    "        if (i*l+l) >= len(source):\n",
    "            break\n",
    "        sentences = source[i*l:i*l+l]\n",
    "        X = [padding(sentence, s_pad, max_length=max_length) for sentence in sentences]        \n",
    "        answers = Y[i*l:i*l+l]\n",
    "        y = []\n",
    "        for j, answer in enumerate(answers):\n",
    "            answer = padding(answer, y_pad, max_length=max_length)\n",
    "            row = np.array([k for k in range(max_length)])\n",
    "            col = np.array([answer[k] for k in range(max_length)])\n",
    "            data = np.array([1.0 for _ in range(max_length)])\n",
    "            m = csr_matrix((data, (row, col)), shape=(max_length, nclasses))\n",
    "            y.append(m.toarray())\n",
    "        \n",
    "        yield (np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_data(n_batch):\n",
    "    g = gen_data(train_lex, idx_pad, train_y, y_pad, max_sentence, vocsize, nclasses, n_batch) \n",
    "    for i in g:\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_data(n_batch):\n",
    "    g = gen_data(test_lex, idx_pad, test_y, y_pad, max_sentence, vocsize, nclasses, n_batch)\n",
    "    for i in g:\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create placeholders for X and y\n",
    "\n",
    "input_x = tf.placeholder(tf.int32, shape=[batch_size, max_sentence])\n",
    "input_y = tf.placeholder(tf.float32, shape=[batch_size, max_sentence, nclasses])\n",
    "\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(\n",
    "        tf.random_uniform([vocsize, embedding_size], -1.0, 1.0),\n",
    "        name=\"W\")\n",
    "    embedded_chars = tf.nn.embedding_lookup(W, input_x) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(embedding_size) \n",
    "\n",
    "inputs = tf.split(embedded_chars, max_sentence, 1)\n",
    "inputs = [tf.reshape(i, shape=[batch_size, -1]) for i in inputs]\n",
    "outputs, states =  tf.contrib.rnn.static_rnn(cell, inputs, dtype = tf.float32)\n",
    "\n",
    "W_o = tf.Variable(tf.random_normal([embedding_size, nclasses], stddev=0.01), name = 'W_o') \n",
    "b_o = tf.Variable(tf.random_normal([nclasses], stddev=0.01), name = 'b_o')\n",
    "\n",
    "outputs3 = [tf.matmul(output, W_o) + b_o for output in outputs] \n",
    "\n",
    "y_answers = tf.split(input_y, max_sentence, 1)\n",
    "y_answers2 = [tf.reshape(i, shape=[batch_size, -1]) for i in y_answers]\n",
    "\n",
    "all_outputs = tf.concat(outputs3,0) #activations \n",
    "all_answers = tf.concat(y_answers2, 0) #labels \n",
    "\n",
    "losses = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = all_answers, logits = all_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"saved_models/model.ckpt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Validation Data\n",
    "test_data_gen = gen_test_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = next(test_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execute\n",
    "import pdb; \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init) #initialize all variables in the model \n",
    "    \n",
    "    for k in range(20):\n",
    "        \n",
    "        #Generate Data for each epoch\n",
    "        #What this does is it creates a list of of elements of length seq_len, each of size [batch_size,input_size]\n",
    "        #this is required to feed data into rnn.rnn\n",
    "        train_gen = gen_train_data(50)\n",
    "        test_gen = gen_test_data(50)\n",
    "        for tr, te in zip(train_gen, test_gen):\n",
    "            X = tr[0]\n",
    "            y = tr[1]\n",
    "            \n",
    "            X_test = te[0]\n",
    "            y_test = te[1]\n",
    "            # pdb.set_trace()\n",
    "            #Create the dictionary of inputs to feed into sess.run\n",
    "            train_dict = {\n",
    "                input_x: X, # [batch_size, max_sentence]\n",
    "                input_y: y# [batch_size, max_sentence, nclasses]\n",
    "            }\n",
    "            \n",
    "\n",
    "            sess.run(optimizer, feed_dict=train_dict)   #perform an update on the parameters   \n",
    "            \n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % save_path) \n",
    "    \n",
    "    test_gen = gen_test_data(50)\n",
    "    \n",
    "    #create validation dictionary\n",
    "    test_dict = {\n",
    "                input_x: X_test, # [batch_size, max_sentence]\n",
    "                input_y: y_test# [batch_size, max_sentence, nclasses]\n",
    "            } \n",
    "    outputs, c_val = sess.run([all_outputs, losses], feed_dict=test_dict)                   \n",
    "    print(\"Validation cost: {}, on Epoch {}\".format(c_val, k)) \n",
    "    \n",
    "    if k >= 100:\n",
    "        pred_1st = np.argmax(outputs[::max_sentence], axis=1)\n",
    "        pred_1st = [idx2label.get(i) for i in pred_1st]\n",
    "\n",
    "        word_1st = [idx2word.get(i) for i in X_test[0]]\n",
    "        y_1st = np.argmax(y_test[0], axis=1)\n",
    "        y_1st = [idx2label.get(i) for i in y_1st]\n",
    "        # pdb.set_trace()\n",
    "        wlength = 7\n",
    "        for w, a, p in zip(word_1st, y_1st, pred_1st):\n",
    "            print(w.rjust(wlength), a.rjust(wlength), p.rjust(wlength))\n",
    "        print('\\n'+'**'*30+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    model_saver.restore(session, \"saved_models/CNN_New.ckpt\")\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized') \n",
    "    \n",
    "    W_o = session.run(W_o)\n",
    "    print(W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Restore latest checkpoint\n",
    "    model_saver.restore(sess, tf.train.latest_checkpoint('saved_model/.'))\n",
    "\n",
    "    # Initalize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Get default graph (supply your custom graph if you have one)\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    # It will give tensor object\n",
    "    b_o = graph.get_tensor_by_name('b_o:0')\n",
    "\n",
    "    # To get the value (numpy array)\n",
    "    b_o_value = session.run(b_o) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
